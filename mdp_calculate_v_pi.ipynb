{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-22T21:14:18.203608Z",
     "start_time": "2024-12-22T21:14:18.200803Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "import jax.lax as lax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from make_mdp import MDP\n"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T21:14:18.218245Z",
     "start_time": "2024-12-22T21:14:18.214144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_states = 100\n",
    "num_actions = 20\n",
    "num_rewards = 10\n",
    "\n",
    "discount_factor = 0.9\n",
    "\n",
    "seed = 42"
   ],
   "id": "d6c5c9a9b53e9bca",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T21:14:18.229574Z",
     "start_time": "2024-12-22T21:14:18.225624Z"
    }
   },
   "cell_type": "code",
   "source": "key = random.key(seed)",
   "id": "b506dcd914304ff6",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T21:14:18.597740Z",
     "start_time": "2024-12-22T21:14:18.238618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#make the MDP\n",
    "key, subkey = random.split(key)\n",
    "mdp = MDP(subkey, num_states, num_actions, num_rewards)\n",
    "del subkey"
   ],
   "id": "11d82b656f9ef048",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T21:14:18.708374Z",
     "start_time": "2024-12-22T21:14:18.615934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#policy matrix: num_states X num_actions with sum(policy[i,:]) == 1\n",
    "key, subkey = random.split(key)\n",
    "policy = random.uniform(subkey, [num_states, num_actions])\n",
    "del subkey\n",
    "\n",
    "policy = policy / policy.sum(axis=-1, keepdims=True)"
   ],
   "id": "f6c7476810766157",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Iterative Policy Evaluation",
   "id": "2796c2f4f90eef58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T21:14:18.714713Z",
     "start_time": "2024-12-22T21:14:18.711885Z"
    }
   },
   "cell_type": "code",
   "source": "eps = 10**(-5)",
   "id": "5ca9f4939be64a8e",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T21:14:18.786799Z",
     "start_time": "2024-12-22T21:14:18.720699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#random initialization\n",
    "key, subkey = random.split(key)\n",
    "v_pi_0 = random.uniform(subkey, num_states, dtype=jnp.float32)"
   ],
   "id": "4fac072b8562cdeb",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e4205b164012adcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In Sutton and Barto we have the update rule (p.74):\n",
    "$$v_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_k(s')]$$\n",
    "\n",
    "Since our MDP is not defined via the four argument probability $p(s',r|s,a)$ but rather state transitions $p(s'|s,a)$ and reward probabilities $p(r|s,a)$ we rewrite this as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{k+1}(s) &= \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_k(s')]\\\\\n",
    "           &= \\sum_a \\pi(a|s) [\\sum_{s',r} p(s',r|s,a) r + \\gamma \\sum_{s',r} p(s',r|s,a) v_k(s')]\\\\\n",
    "           &= \\sum_a \\pi(a|s) [\\sum_{r} r \\sum_{s'} p(s',r|s,a) + \\gamma \\sum_{s'} v_k(s') sum_{r} p(s',r|s,a) ]\\\\\n",
    "           &= \\sum_a \\pi(a|s) [\\sum_{r} r p(r|s,a) + \\gamma \\sum_{s'} v_k(s') p(s'|s,a) ]\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\sum_{r} r p(r|s,a)$ is simply the expected reward when choosing action $a$ at state $s$. This is pre-computed as `mdp.expected_rewards`.\n",
    "$\\gamma \\sum_{s'} v_k(s') p(s'|s,a)$ is the discounted expectation of the value over the next states when choosing action $a$. \n"
   ],
   "id": "82e844db8c885058"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Simple while-loop implementation (synchronous updates)",
   "id": "ae5a01313f2bdd7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T21:15:31.282516Z",
     "start_time": "2024-12-22T21:14:18.795430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "v_pi = jnp.copy(v_pi_0)\n",
    "num_iterations = 0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "while True:\n",
    "    num_iterations += 1\n",
    "    \n",
    "    delta = 0\n",
    "    v_pi_old = jnp.copy(v_pi)\n",
    "    for s in range(num_states):\n",
    "        v_pi_s = 0\n",
    "        for a in range(num_actions):\n",
    "            policy_s_a = policy[s, a]\n",
    "            v_pi_s += policy_s_a * mdp.expected_rewards[a, s]\n",
    "            v_pi_s += policy_s_a * discount_factor * jnp.dot(v_pi_old, mdp.transition_ps[s, a, :])\n",
    "        v_pi = v_pi.at[s].set(v_pi_s)\n",
    "\n",
    "        delta = max(delta, jnp.abs(v_pi_old[s] - v_pi[s]))\n",
    "    \n",
    "    if delta < eps:\n",
    "        break\n",
    "\n",
    "_ = v_pi.block_until_ready()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "elapsed_loop = end - start\n",
    "print(f\"Iterations {num_iterations} - Delta {delta:.5f} - Time: {elapsed_loop:.6f} seconds\")"
   ],
   "id": "9e7643b87b7bc21f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 102 - Delta 0.00001 - Time: 72.472999 seconds\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lax-While Loop",
   "id": "dcb4b0b4d234acf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T21:15:53.261207Z",
     "start_time": "2024-12-22T21:15:31.359196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def iterative_value_estimation(state):\n",
    "    delta, v_pi = state\n",
    "    delta = 0\n",
    "    v_pi_old = jnp.copy(v_pi)\n",
    "    for s in range(num_states):\n",
    "        v_pi_s = 0\n",
    "        for a in range(num_actions):\n",
    "            policy_s_a = policy[s, a]\n",
    "            v_pi_s += policy_s_a * mdp.expected_rewards[a, s]\n",
    "            v_pi_s += policy_s_a * discount_factor * jnp.dot(v_pi_old, mdp.transition_ps[s, a, :])\n",
    "        v_pi = v_pi.at[s].set(v_pi_s)\n",
    "\n",
    "        delta = jnp.maximum(delta, jnp.abs(v_pi_old[s] - v_pi[s]))\n",
    "    return (delta, v_pi)\n",
    "\n",
    "def cond_function(state):\n",
    "    delta, v_pi = state\n",
    "    return delta > eps\n",
    "\n",
    "v_pi = jnp.copy(v_pi_0)\n",
    "init_state = (1e13, v_pi)\n",
    "\n",
    "start = time.time()\n",
    "lax.while_loop(cond_function, iterative_value_estimation, init_state)\n",
    "end = time.time()\n",
    "elapsed_lax_loop = end - start\n",
    "print(f\"Iterations {num_iterations} - Delta {delta:.5f} - Time: {elapsed_lax_loop:.6f} seconds\")"
   ],
   "id": "9825982451a87a29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 102 - Delta 0.00001 - Time: 21.894257 seconds\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vectorized update in lax loop\n",
   "id": "2eda50252b24accd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T21:21:03.043583Z",
     "start_time": "2024-12-22T21:21:02.894100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def vectorized_value_estimation(state):\n",
    "    delta, v_pi = state\n",
    "    expected_r = jnp.sum(policy * mdp.expected_rewards, axis=1)\n",
    "    expected_v = jnp.sum(policy[:, :, None] * mdp.transition_ps * v_pi[None, None, :], axis=(1,2)) \n",
    "    v_pi_new = expected_r + discount_factor * expected_v\n",
    "    delta = jnp.max(jnp.abs(v_pi_new - v_pi))\n",
    "    return (delta, v_pi_new)\n",
    "\n",
    "def cond_function(state):\n",
    "    delta, v_pi = state\n",
    "    return delta > eps\n",
    "\n",
    "\n",
    "v_pi = jnp.copy(v_pi_0)\n",
    "init_state = (1e13, v_pi)\n",
    "\n",
    "start = time.time()\n",
    "lax.while_loop(cond_function, vectorized_value_estimation, init_state)\n",
    "end = time.time()\n",
    "elapsed_vectorized_lax_loop = end - start\n",
    "print(f\"Iterations {num_iterations} - Delta {delta:.5f} - Time: {elapsed_vectorized_lax_loop:.6f} seconds\")"
   ],
   "id": "f30631d92e51bef3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 102 - Delta 0.00001 - Time: 0.133021 seconds\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T21:21:08.607718Z",
     "start_time": "2024-12-22T21:21:08.604722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'Simple Loop: {elapsed_loop:.6f} seconds')\n",
    "print(f'Lax Loop: {elapsed_lax_loop:.6f} seconds')\n",
    "print(f'Vectorized Lax Loop: {elapsed_vectorized_lax_loop:.6f} seconds')"
   ],
   "id": "4ed5f4653a8dc53c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Loop: 72.472999 seconds\n",
      "Lax Loop: 21.894257 seconds\n",
      "Vectorized Lax Loop: 0.133021 seconds\n"
     ]
    }
   ],
   "execution_count": 46
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
