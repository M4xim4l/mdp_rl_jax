{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-22T22:54:18.809471Z",
     "start_time": "2024-12-22T22:54:18.742106Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "import jax.lax as lax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from make_mdp import MDP\n"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:54:18.851690Z",
     "start_time": "2024-12-22T22:54:18.842284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_states = 30\n",
    "num_actions = 15\n",
    "num_rewards = 10\n",
    "\n",
    "reward_mean = 0.0\n",
    "reward_std = 10.0\n",
    "\n",
    "discount_factor = 0.9\n",
    "\n",
    "seed = 42"
   ],
   "id": "d6c5c9a9b53e9bca",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:54:18.954038Z",
     "start_time": "2024-12-22T22:54:18.871247Z"
    }
   },
   "cell_type": "code",
   "source": "key = random.key(seed)",
   "id": "b506dcd914304ff6",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:54:20.383052Z",
     "start_time": "2024-12-22T22:54:19.032961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#make the MDP\n",
    "key, subkey = random.split(key)\n",
    "mdp = MDP(subkey, num_states, num_actions, num_rewards, reward_mean, reward_std)\n",
    "del subkey"
   ],
   "id": "11d82b656f9ef048",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:54:20.566685Z",
     "start_time": "2024-12-22T22:54:20.399185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#policy matrix: num_states X num_actions with sum(policy[i,:]) == 1\n",
    "key, subkey = random.split(key)\n",
    "policy = random.uniform(subkey, [num_states, num_actions])\n",
    "del subkey\n",
    "\n",
    "policy = policy / policy.sum(axis=-1, keepdims=True)"
   ],
   "id": "f6c7476810766157",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Iterative Policy Evaluation",
   "id": "2796c2f4f90eef58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:54:20.584116Z",
     "start_time": "2024-12-22T22:54:20.580714Z"
    }
   },
   "cell_type": "code",
   "source": "eps = 10**(-8)",
   "id": "5ca9f4939be64a8e",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:54:20.698121Z",
     "start_time": "2024-12-22T22:54:20.595134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#random initialization\n",
    "key, subkey = random.split(key)\n",
    "v_pi_0 = random.uniform(subkey, num_states, dtype=jnp.float32)"
   ],
   "id": "4fac072b8562cdeb",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In Sutton and Barto we have the update rule (p.74):\n",
    "$$v_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_k(s')]$$\n",
    "\n",
    "Since our MDP is not defined via the four argument probability $p(s',r|s,a)$ but rather state transitions $p(s'|s,a)$ and reward probabilities $p(r|s,a)$ we rewrite this as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{k+1}(s) &= \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_k(s')]\\\\\n",
    "           &= \\sum_a \\pi(a|s) [\\sum_{s',r} p(s',r|s,a) r + \\gamma \\sum_{s',r} p(s',r|s,a) v_k(s')]\\\\\n",
    "           &= \\sum_a \\pi(a|s) [\\sum_{r} r \\sum_{s'} p(s',r|s,a) + \\gamma \\sum_{s'} v_k(s') \\sum_{r} p(s',r|s,a) ]\\\\\n",
    "           &= \\sum_a \\pi(a|s) [\\sum_{r} r p(r|s,a) + \\gamma \\sum_{s'} v_k(s') p(s'|s,a) ]\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\sum_{r} r p(r|s,a)$ is simply the expected reward when choosing action $a$ at state $s$. This is pre-computed as `mdp.expected_rewards`.\n",
    "$\\gamma \\sum_{s'} v_k(s') p(s'|s,a)$ is the discounted expectation of the value over the next states when choosing action $a$. \n"
   ],
   "id": "82e844db8c885058"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Simple while-loop implementation (synchronous updates)",
   "id": "ae5a01313f2bdd7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:54:40.780095Z",
     "start_time": "2024-12-22T22:54:20.725149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "v_pi = jnp.copy(v_pi_0)\n",
    "num_iterations = 0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "while True:\n",
    "    num_iterations += 1\n",
    "    \n",
    "    delta = 0\n",
    "    v_pi_old = jnp.copy(v_pi)\n",
    "    for s in range(num_states):\n",
    "        v_pi_s = 0\n",
    "        for a in range(num_actions):\n",
    "            policy_s_a = policy[s, a]\n",
    "            v_pi_s += policy_s_a * mdp.expected_rewards[s,a]\n",
    "            v_pi_s += policy_s_a * discount_factor * jnp.dot(v_pi_old, mdp.transition_ps[s, a, :])\n",
    "        v_pi = v_pi.at[s].set(v_pi_s)\n",
    "\n",
    "        delta = max(delta, jnp.abs(v_pi_old[s] - v_pi[s]))\n",
    "    \n",
    "    if delta < eps:\n",
    "        break\n",
    "\n",
    "_ = v_pi.block_until_ready()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "elapsed_loop = end - start\n",
    "print(f\"Iterations {num_iterations} - Delta {delta:.5f} - Time: {elapsed_loop:.6f} seconds\")\n",
    "print(f\"Value function:\\n{v_pi}\")\n"
   ],
   "id": "9e7643b87b7bc21f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 151 - Delta 0.00000 - Time: 20.032519 seconds\n",
      "Value function:\n",
      "[-3.35251   -3.3723989 -3.3082206 -3.3765604 -3.371306  -3.4147203\n",
      " -3.476385  -3.41075   -3.4429617 -3.3209078 -3.4015646 -3.35638\n",
      " -3.410884  -3.3508618 -3.3867407 -3.3969636 -3.3065453 -3.3682504\n",
      " -3.3791513 -3.4573994 -3.4402492 -3.4397793 -3.4604473 -3.289907\n",
      " -3.3306336 -3.2623203 -3.434849  -3.3700235 -3.4616327 -3.424651 ]\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lax-While Loop",
   "id": "dcb4b0b4d234acf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:54:44.997315Z",
     "start_time": "2024-12-22T22:54:40.890762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def iterative_value_estimation(state):\n",
    "    delta, v_pi, iteration = state\n",
    "    delta = 0\n",
    "    iteration += 1\n",
    "    v_pi_old = jnp.copy(v_pi)\n",
    "    for s in range(num_states):\n",
    "        v_pi_s = 0\n",
    "        for a in range(num_actions):\n",
    "            policy_s_a = policy[s, a]\n",
    "            v_pi_s += policy_s_a * mdp.expected_rewards[s,a]\n",
    "            v_pi_s += policy_s_a * discount_factor * jnp.dot(v_pi_old, mdp.transition_ps[s, a, :])\n",
    "        v_pi = v_pi.at[s].set(v_pi_s)\n",
    "\n",
    "        delta = jnp.maximum(delta, jnp.abs(v_pi_old[s] - v_pi[s]))\n",
    "    return (delta, v_pi, iteration)\n",
    "\n",
    "def cond_function(state):\n",
    "    delta, v_pi, _ = state\n",
    "    return delta > eps\n",
    "\n",
    "v_pi = jnp.copy(v_pi_0)\n",
    "init_state = (1e13, v_pi, 0)\n",
    "\n",
    "start = time.time()\n",
    "delta, v_pi, num_iterations = lax.while_loop(cond_function, iterative_value_estimation, init_state)\n",
    "end = time.time()\n",
    "\n",
    "elapsed_lax_loop = end - start\n",
    "print(f\"Iterations {num_iterations} - Delta {delta:.5f} - Time: {elapsed_lax_loop:.6f} seconds\")\n",
    "print(f\"Value function:\\n{v_pi}\")"
   ],
   "id": "9825982451a87a29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 161 - Delta 0.00000 - Time: 4.081857 seconds\n",
      "Value function:\n",
      "[-3.3525105 -3.372399  -3.3082213 -3.3765607 -3.3713064 -3.4147208\n",
      " -3.4763856 -3.4107502 -3.4429622 -3.3209085 -3.4015653 -3.3563802\n",
      " -3.4108846 -3.350862  -3.3867414 -3.3969638 -3.3065462 -3.3682508\n",
      " -3.3791518 -3.4573998 -3.4402494 -3.4397798 -3.4604475 -3.2899072\n",
      " -3.3306339 -3.2623205 -3.4348495 -3.370024  -3.4616337 -3.424651 ]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vectorized update in lax loop\n",
   "id": "2eda50252b24accd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:54:45.061915Z",
     "start_time": "2024-12-22T22:54:45.010529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def vectorized_value_estimation(state):\n",
    "    delta, v_pi, iteration = state\n",
    "    iteration += 1\n",
    "    expected_r = jnp.sum(policy * mdp.expected_rewards, axis=1)\n",
    "    expected_v = jnp.sum(policy[:, :, None] * mdp.transition_ps * v_pi[None, None, :], axis=(1,2)) \n",
    "    v_pi_new = expected_r + discount_factor * expected_v\n",
    "    delta = jnp.max(jnp.abs(v_pi_new - v_pi))\n",
    "    return (delta, v_pi_new, iteration)\n",
    "\n",
    "def cond_function(state):\n",
    "    delta, v_pi, _ = state\n",
    "    return delta > eps\n",
    "\n",
    "v_pi = jnp.copy(v_pi_0)\n",
    "init_state = (1e13, v_pi, 0)\n",
    "\n",
    "start = time.time()\n",
    "delta, v_pi, num_iterations = lax.while_loop(cond_function, vectorized_value_estimation, init_state)\n",
    "end = time.time()\n",
    "\n",
    "elapsed_vectorized_lax_loop = end - start\n",
    "print(f\"Iterations {num_iterations} - Delta {delta:.5f} - Time: {elapsed_vectorized_lax_loop:.6f} seconds\")\n",
    "print(f\"Value function:\\n{v_pi}\")"
   ],
   "id": "f30631d92e51bef3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 153 - Delta 0.00000 - Time: 0.045203 seconds\n",
      "Value function:\n",
      "[-3.3525121 -3.3724003 -3.3082228 -3.3765624 -3.3713074 -3.4147222\n",
      " -3.4763868 -3.4107516 -3.4429636 -3.320909  -3.401567  -3.3563805\n",
      " -3.410886  -3.350864  -3.3867414 -3.396964  -3.3065472 -3.36825\n",
      " -3.3791535 -3.4574013 -3.4402509 -3.4397807 -3.4604495 -3.2899094\n",
      " -3.3306346 -3.2623215 -3.4348507 -3.370025  -3.461634  -3.424654 ]\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Alternatively, we can directly solve the underlying linear system.\n",
    "The original Bellman equation was:\n",
    "\n",
    "$$v_\\pi(s) = \\sum_a \\pi(a|s) [\\sum_r r p(r|s,a) + \\gamma \\sum_{s'} v_\\pi(s') p(s'|s,a)]$$\n",
    "\n",
    "Which we can rewrite as:\n",
    "\n",
    "$$\\sum_a \\pi(a|s) \\sum_r r p(r|s,a) = v_\\pi(s) - \\gamma \\sum_{s'} v_\\pi(s') \\sum_a \\pi(a|s) p(s'|s,a)$$\n",
    "\n",
    "For a finite MDP, this can be rewritten as the linear system:\n",
    "\n",
    "$$\\mathbf{r} = (\\mathbf{I} - \\gamma \\mathbf{P}) \\mathbf{v}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{r}_i = \\sum_a \\pi(a|s_i) \\sum_r r p(r|s_i,a)\\\\\n",
    "    \\mathbf{P}_{i,j} = \\sum_a \\pi(a|s_i) p(s_j|s,a) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    " (we use $s_i$ to denote the $i$-th state)."
   ],
   "id": "b5fb3907df0debd3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:54:45.445105Z",
     "start_time": "2024-12-22T22:54:45.075950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "r = jnp.sum(policy * mdp.expected_rewards, axis=1)\n",
    "P = jnp.sum( policy[:, :, None] * mdp.transition_ps, axis=1  )\n",
    "v_pi = jnp.linalg.solve(jnp.eye(num_states) - discount_factor * P, r)\n",
    "end = time.time()\n",
    "elapsed_linear_system = end - start\n",
    "print(f\"Time: {elapsed_linear_system:.6f} seconds\")\n",
    "print(f\"Value function:\\n{v_pi}\")\n"
   ],
   "id": "123ef42b1514215d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.360672 seconds\n",
      "Value function:\n",
      "[-3.35251   -3.3723993 -3.308222  -3.376561  -3.3713067 -3.4147213\n",
      " -3.4763858 -3.4107506 -3.4429624 -3.3209093 -3.4015653 -3.3563802\n",
      " -3.4108849 -3.350863  -3.3867414 -3.3969636 -3.3065467 -3.3682516\n",
      " -3.379152  -3.4574    -3.4402497 -3.43978   -3.4604478 -3.2899075\n",
      " -3.3306339 -3.2623205 -3.4348502 -3.3700242 -3.461634  -3.4246514]\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:54:45.461804Z",
     "start_time": "2024-12-22T22:54:45.458612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'Simple Loop: {elapsed_loop:.6f} seconds')\n",
    "print(f'Lax Loop: {elapsed_lax_loop:.6f} seconds')\n",
    "print(f'Vectorized Lax Loop: {elapsed_vectorized_lax_loop:.6f} seconds')\n",
    "print(f'Linear system: {elapsed_linear_system:.6f} seconds')"
   ],
   "id": "4ed5f4653a8dc53c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Loop: 20.032519 seconds\n",
      "Lax Loop: 4.081857 seconds\n",
      "Vectorized Lax Loop: 0.045203 seconds\n",
      "Linear system: 0.360672 seconds\n"
     ]
    }
   ],
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
