{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-22T22:27:03.633602Z",
     "start_time": "2024-12-22T22:27:03.630076Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "import jax.lax as lax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from make_mdp import MDP\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:27:03.648651Z",
     "start_time": "2024-12-22T22:27:03.645836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_states = 100\n",
    "num_actions = 20\n",
    "num_rewards = 10\n",
    "\n",
    "discount_factor = 0.9\n",
    "\n",
    "seed = 42"
   ],
   "id": "d6c5c9a9b53e9bca",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:27:03.662910Z",
     "start_time": "2024-12-22T22:27:03.657326Z"
    }
   },
   "cell_type": "code",
   "source": "key = random.key(seed)",
   "id": "b506dcd914304ff6",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:27:03.682305Z",
     "start_time": "2024-12-22T22:27:03.671086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#make the MDP\n",
    "key, subkey = random.split(key)\n",
    "mdp = MDP(subkey, num_states, num_actions, num_rewards)\n",
    "del subkey"
   ],
   "id": "11d82b656f9ef048",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:27:03.696488Z",
     "start_time": "2024-12-22T22:27:03.692342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#policy matrix: num_states X num_actions with sum(policy[i,:]) == 1\n",
    "key, subkey = random.split(key)\n",
    "policy = random.uniform(subkey, [num_states, num_actions])\n",
    "del subkey\n",
    "\n",
    "policy = policy / policy.sum(axis=-1, keepdims=True)"
   ],
   "id": "f6c7476810766157",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Iterative Policy Evaluation",
   "id": "2796c2f4f90eef58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:27:03.707951Z",
     "start_time": "2024-12-22T22:27:03.704872Z"
    }
   },
   "cell_type": "code",
   "source": "eps = 10**(-8)",
   "id": "5ca9f4939be64a8e",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:27:03.719428Z",
     "start_time": "2024-12-22T22:27:03.716142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#random initialization\n",
    "key, subkey = random.split(key)\n",
    "v_pi_0 = random.uniform(subkey, num_states, dtype=jnp.float32)"
   ],
   "id": "4fac072b8562cdeb",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In Sutton and Barto we have the update rule (p.74):\n",
    "$$v_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_k(s')]$$\n",
    "\n",
    "Since our MDP is not defined via the four argument probability $p(s',r|s,a)$ but rather state transitions $p(s'|s,a)$ and reward probabilities $p(r|s,a)$ we rewrite this as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{k+1}(s) &= \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_k(s')]\\\\\n",
    "           &= \\sum_a \\pi(a|s) [\\sum_{s',r} p(s',r|s,a) r + \\gamma \\sum_{s',r} p(s',r|s,a) v_k(s')]\\\\\n",
    "           &= \\sum_a \\pi(a|s) [\\sum_{r} r \\sum_{s'} p(s',r|s,a) + \\gamma \\sum_{s'} v_k(s') \\sum_{r} p(s',r|s,a) ]\\\\\n",
    "           &= \\sum_a \\pi(a|s) [\\sum_{r} r p(r|s,a) + \\gamma \\sum_{s'} v_k(s') p(s'|s,a) ]\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\sum_{r} r p(r|s,a)$ is simply the expected reward when choosing action $a$ at state $s$. This is pre-computed as `mdp.expected_rewards`.\n",
    "$\\gamma \\sum_{s'} v_k(s') p(s'|s,a)$ is the discounted expectation of the value over the next states when choosing action $a$. \n"
   ],
   "id": "82e844db8c885058"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Simple while-loop implementation (synchronous updates)",
   "id": "ae5a01313f2bdd7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:28:23.439498Z",
     "start_time": "2024-12-22T22:27:03.728992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "v_pi = jnp.copy(v_pi_0)\n",
    "num_iterations = 0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "while True:\n",
    "    num_iterations += 1\n",
    "    \n",
    "    delta = 0\n",
    "    v_pi_old = jnp.copy(v_pi)\n",
    "    for s in range(num_states):\n",
    "        v_pi_s = 0\n",
    "        for a in range(num_actions):\n",
    "            policy_s_a = policy[s, a]\n",
    "            v_pi_s += policy_s_a * mdp.expected_rewards[s,a]\n",
    "            v_pi_s += policy_s_a * discount_factor * jnp.dot(v_pi_old, mdp.transition_ps[s, a, :])\n",
    "        v_pi = v_pi.at[s].set(v_pi_s)\n",
    "\n",
    "        delta = max(delta, jnp.abs(v_pi_old[s] - v_pi[s]))\n",
    "    \n",
    "    if delta < eps:\n",
    "        break\n",
    "\n",
    "_ = v_pi.block_until_ready()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "elapsed_loop = end - start\n",
    "print(f\"Iterations {num_iterations} - Delta {delta:.5f} - Time: {elapsed_loop:.6f} seconds\")\n",
    "print(f\"Value function:\\n{v_pi}\")\n"
   ],
   "id": "9e7643b87b7bc21f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 167 - Delta 0.00000 - Time: 79.704119 seconds\n",
      "Value function:\n",
      "[-3.2663145 -3.329989  -3.3181143 -3.3560846 -3.396226  -3.3184915\n",
      " -3.2981966 -3.2488885 -3.2935884 -3.3894997 -3.2358794 -3.3393958\n",
      " -3.2019312 -3.2801948 -3.352533  -3.3263695 -3.3368227 -3.2322514\n",
      " -3.3093536 -3.309447  -3.283536  -3.3124564 -3.2621002 -3.3357408\n",
      " -3.2462966 -3.3823938 -3.3462186 -3.285872  -3.3370242 -3.247308\n",
      " -3.2328722 -3.219432  -3.3058698 -3.4036813 -3.2634966 -3.308061\n",
      " -3.3138328 -3.260548  -3.319434  -3.1981363 -3.3334718 -3.2856119\n",
      " -3.2657304 -3.302262  -3.349907  -3.3335283 -3.3454852 -3.3032675\n",
      " -3.3417597 -3.3143177 -3.3914058 -3.3519425 -3.293618  -3.324379\n",
      " -3.223511  -3.3436432 -3.304626  -3.4594944 -3.2967875 -3.2219634\n",
      " -3.261729  -3.3466275 -3.3195114 -3.3208437 -3.3270507 -3.334924\n",
      " -3.2326894 -3.3307955 -3.3029542 -3.32861   -3.2193573 -3.2928288\n",
      " -3.3427389 -3.3128898 -3.2813003 -3.3573318 -3.2608452 -3.2219803\n",
      " -3.3352835 -3.348486  -3.357401  -3.2815063 -3.3321888 -3.3096223\n",
      " -3.384559  -3.2686403 -3.2243903 -3.3803275 -3.3044744 -3.3111753\n",
      " -3.2461083 -3.315918  -3.3691945 -3.3521583 -3.3177054 -3.2874498\n",
      " -3.2795053 -3.4098458 -3.196888  -3.310639 ]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lax-While Loop",
   "id": "dcb4b0b4d234acf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:28:47.593103Z",
     "start_time": "2024-12-22T22:28:23.450185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def iterative_value_estimation(state):\n",
    "    delta, v_pi, iteration = state\n",
    "    delta = 0\n",
    "    iteration += 1\n",
    "    v_pi_old = jnp.copy(v_pi)\n",
    "    for s in range(num_states):\n",
    "        v_pi_s = 0\n",
    "        for a in range(num_actions):\n",
    "            policy_s_a = policy[s, a]\n",
    "            v_pi_s += policy_s_a * mdp.expected_rewards[s,a]\n",
    "            v_pi_s += policy_s_a * discount_factor * jnp.dot(v_pi_old, mdp.transition_ps[s, a, :])\n",
    "        v_pi = v_pi.at[s].set(v_pi_s)\n",
    "\n",
    "        delta = jnp.maximum(delta, jnp.abs(v_pi_old[s] - v_pi[s]))\n",
    "    return (delta, v_pi, iteration)\n",
    "\n",
    "def cond_function(state):\n",
    "    delta, v_pi, _ = state\n",
    "    return delta > eps\n",
    "\n",
    "v_pi = jnp.copy(v_pi_0)\n",
    "init_state = (1e13, v_pi, 0)\n",
    "\n",
    "start = time.time()\n",
    "delta, v_pi, num_iterations = lax.while_loop(cond_function, iterative_value_estimation, init_state)\n",
    "end = time.time()\n",
    "\n",
    "elapsed_lax_loop = end - start\n",
    "print(f\"Iterations {num_iterations} - Delta {delta:.5f} - Time: {elapsed_lax_loop:.6f} seconds\")\n",
    "print(f\"Value function:\\n{v_pi}\")"
   ],
   "id": "9825982451a87a29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 164 - Delta 0.00000 - Time: 24.065244 seconds\n",
      "Value function:\n",
      "[-3.2663147 -3.3299887 -3.318114  -3.3560846 -3.396226  -3.3184915\n",
      " -3.2981966 -3.2488885 -3.2935884 -3.3894997 -3.2358794 -3.3393958\n",
      " -3.2019312 -3.2801945 -3.352533  -3.3263695 -3.3368227 -3.2322514\n",
      " -3.3093536 -3.309447  -3.2835362 -3.3124564 -3.2621    -3.3357408\n",
      " -3.2462966 -3.3823936 -3.3462186 -3.285872  -3.3370242 -3.247308\n",
      " -3.2328722 -3.2194319 -3.3058698 -3.4036813 -3.2634964 -3.308061\n",
      " -3.3138328 -3.260548  -3.3194335 -3.198136  -3.333472  -3.2856119\n",
      " -3.2657304 -3.302262  -3.3499074 -3.3335283 -3.3454852 -3.3032675\n",
      " -3.3417594 -3.3143177 -3.3914058 -3.3519425 -3.2936177 -3.3243787\n",
      " -3.2235112 -3.3436432 -3.304626  -3.459494  -3.2967875 -3.2219636\n",
      " -3.261729  -3.3466275 -3.3195114 -3.3208437 -3.3270507 -3.334924\n",
      " -3.2326896 -3.3307953 -3.3029542 -3.3286095 -3.2193573 -3.2928288\n",
      " -3.3427389 -3.3128898 -3.2812996 -3.3573318 -3.2608452 -3.22198\n",
      " -3.3352835 -3.348486  -3.3574011 -3.2815063 -3.3321886 -3.309622\n",
      " -3.384559  -3.2686403 -3.2243903 -3.3803275 -3.3044744 -3.3111753\n",
      " -3.246108  -3.315918  -3.3691947 -3.3521583 -3.3177054 -3.2874496\n",
      " -3.2795055 -3.4098458 -3.1968882 -3.3106387]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vectorized update in lax loop\n",
   "id": "2eda50252b24accd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:28:47.673611Z",
     "start_time": "2024-12-22T22:28:47.603875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def vectorized_value_estimation(state):\n",
    "    delta, v_pi, iteration = state\n",
    "    iteration += 1\n",
    "    expected_r = jnp.sum(policy * mdp.expected_rewards, axis=1)\n",
    "    expected_v = jnp.sum(policy[:, :, None] * mdp.transition_ps * v_pi[None, None, :], axis=(1,2)) \n",
    "    v_pi_new = expected_r + discount_factor * expected_v\n",
    "    delta = jnp.max(jnp.abs(v_pi_new - v_pi))\n",
    "    return (delta, v_pi_new, iteration)\n",
    "\n",
    "def cond_function(state):\n",
    "    delta, v_pi, _ = state\n",
    "    return delta > eps\n",
    "\n",
    "v_pi = jnp.copy(v_pi_0)\n",
    "init_state = (1e13, v_pi, 0)\n",
    "\n",
    "start = time.time()\n",
    "delta, v_pi, num_iterations = lax.while_loop(cond_function, vectorized_value_estimation, init_state)\n",
    "end = time.time()\n",
    "\n",
    "elapsed_vectorized_lax_loop = end - start\n",
    "print(f\"Iterations {num_iterations} - Delta {delta:.5f} - Time: {elapsed_vectorized_lax_loop:.6f} seconds\")\n",
    "print(f\"Value function:\\n{v_pi}\")"
   ],
   "id": "f30631d92e51bef3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 166 - Delta 0.00000 - Time: 0.047804 seconds\n",
      "Value function:\n",
      "[-3.2663147 -3.329988  -3.3181143 -3.3560848 -3.3962255 -3.3184915\n",
      " -3.298196  -3.2488887 -3.293588  -3.3895004 -3.2358797 -3.3393962\n",
      " -3.2019315 -3.2801945 -3.352533  -3.3263698 -3.3368225 -3.232251\n",
      " -3.309354  -3.3094468 -3.2835362 -3.3124566 -3.2621007 -3.335741\n",
      " -3.2462964 -3.382394  -3.3462183 -3.2858715 -3.3370245 -3.2473085\n",
      " -3.2328722 -3.219432  -3.3058703 -3.4036813 -3.2634969 -3.3080614\n",
      " -3.3138323 -3.2605484 -3.3194337 -3.1981366 -3.3334718 -3.285612\n",
      " -3.2657304 -3.3022628 -3.3499076 -3.3335285 -3.3454866 -3.3032672\n",
      " -3.3417604 -3.3143175 -3.3914058 -3.3519423 -3.2936182 -3.3243787\n",
      " -3.2235107 -3.3436432 -3.3046265 -3.4594948 -3.2967873 -3.221964\n",
      " -3.2617295 -3.3466275 -3.319511  -3.320844  -3.3270502 -3.3349245\n",
      " -3.2326899 -3.3307958 -3.3029554 -3.3286097 -3.2193568 -3.2928293\n",
      " -3.3427386 -3.31289   -3.2812994 -3.3573322 -3.2608454 -3.2219803\n",
      " -3.3352838 -3.3484857 -3.357401  -3.2815065 -3.3321886 -3.3096225\n",
      " -3.3845587 -3.2686405 -3.22439   -3.3803267 -3.3044746 -3.3111756\n",
      " -3.2461083 -3.3159177 -3.369195  -3.352159  -3.3177056 -3.2874494\n",
      " -3.2795057 -3.409847  -3.1968884 -3.3106394]\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Alternatively, we can directly solve the underlying linear system.\n",
    "The original Bellman equation was:\n",
    "\n",
    "$$v_\\pi(s) = \\sum_a \\pi(a|s) [\\sum_r r p(r|s,a) + \\gamma \\sum_{s'} v_\\pi(s') p(s'|s,a)]$$\n",
    "\n",
    "Which we can rewrite as:\n",
    "\n",
    "$$\\sum_a \\pi(a|s) \\sum_r r p(r|s,a) = v_\\pi(s) - \\gamma \\sum_{s'} v_\\pi(s') \\sum_a \\pi(a|s) p(s'|s,a)$$\n",
    "\n",
    "For a finite MDP, this can be rewritten as the linear system:\n",
    "\n",
    "$$\\mathbf{r} = (\\mathbf{I} - \\gamma \\mathbf{P}) \\mathbf{v}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{r}_i = \\sum_a \\pi(a|s_i) \\sum_r r p(r|s_i,a)\\\\\n",
    "    \\mathbf{P}_{i,j} = \\sum_a \\pi(a|s_i) p(s_j|s,a) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    " (we use $s_i$ to denote the $i$-th state)."
   ],
   "id": "b5fb3907df0debd3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:28:47.689904Z",
     "start_time": "2024-12-22T22:28:47.682357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "r = jnp.sum(policy * mdp.expected_rewards, axis=1)\n",
    "P = jnp.sum( policy[:, :, None] * mdp.transition_ps, axis=1  )\n",
    "v_pi = jnp.linalg.solve(jnp.eye(num_states) - discount_factor * P, r)\n",
    "end = time.time()\n",
    "elapsed_linear_system = end - start\n",
    "print(f\"Time: {elapsed_linear_system:.6f} seconds\")\n",
    "print(f\"Value function:\\n{v_pi}\")\n"
   ],
   "id": "123ef42b1514215d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.002164 seconds\n",
      "Value function:\n",
      "[-3.266314  -3.3299894 -3.3181143 -3.3560843 -3.396226  -3.318492\n",
      " -3.2981966 -3.2488885 -3.2935886 -3.3895006 -3.235879  -3.3393962\n",
      " -3.2019315 -3.280194  -3.3525338 -3.3263705 -3.3368232 -3.2322514\n",
      " -3.309354  -3.309448  -3.2835362 -3.3124561 -3.262101  -3.3357413\n",
      " -3.246297  -3.3823943 -3.3462183 -3.2858713 -3.337024  -3.2473087\n",
      " -3.2328722 -3.2194319 -3.3058708 -3.403682  -3.2634974 -3.3080616\n",
      " -3.3138325 -3.2605488 -3.3194335 -3.1981368 -3.3334723 -3.2856119\n",
      " -3.2657301 -3.3022628 -3.3499079 -3.3335283 -3.3454857 -3.3032675\n",
      " -3.3417597 -3.3143177 -3.3914056 -3.3519423 -3.2936182 -3.3243787\n",
      " -3.223511  -3.3436434 -3.3046255 -3.4594948 -3.2967877 -3.2219641\n",
      " -3.2617297 -3.3466284 -3.3195107 -3.3208444 -3.3270504 -3.3349252\n",
      " -3.23269   -3.3307958 -3.3029552 -3.3286092 -3.2193573 -3.2928288\n",
      " -3.3427396 -3.31289   -3.2813    -3.3573322 -3.2608452 -3.22198\n",
      " -3.3352842 -3.348486  -3.3574007 -3.2815065 -3.3321886 -3.3096225\n",
      " -3.384559  -3.2686405 -3.2243903 -3.3803267 -3.3044744 -3.3111758\n",
      " -3.2461083 -3.3159182 -3.3691947 -3.3521585 -3.3177056 -3.2874491\n",
      " -3.2795055 -3.4098468 -3.1968887 -3.310639 ]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T22:28:47.762912Z",
     "start_time": "2024-12-22T22:28:47.759191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'Simple Loop: {elapsed_loop:.6f} seconds')\n",
    "print(f'Lax Loop: {elapsed_lax_loop:.6f} seconds')\n",
    "print(f'Vectorized Lax Loop: {elapsed_vectorized_lax_loop:.6f} seconds')\n",
    "print(f'Linear system: {elapsed_linear_system:.6f} seconds')"
   ],
   "id": "4ed5f4653a8dc53c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Loop: 79.704119 seconds\n",
      "Lax Loop: 24.065244 seconds\n",
      "Vectorized Lax Loop: 0.047804 seconds\n",
      "Linear system: 0.002164 seconds\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
