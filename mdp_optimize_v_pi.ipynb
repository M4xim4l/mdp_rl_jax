{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-22T23:32:22.033431Z",
     "start_time": "2024-12-22T23:32:21.775013Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "import jax.lax as lax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "\n",
    "from make_mdp import MDP\n",
    "\n",
    "num_states = 100\n",
    "num_actions = 20\n",
    "num_rewards = 10\n",
    "\n",
    "reward_mean = 5.0\n",
    "reward_std = 10.0\n",
    "\n",
    "discount_factor = 0.9\n",
    "\n",
    "seed = 42\n",
    "\n",
    "key = random.key(seed)\n",
    "\n",
    "#make the MDP\n",
    "key, subkey = random.split(key)\n",
    "mdp = MDP(subkey, num_states, num_actions, num_rewards, reward_mean, reward_std)\n",
    "del subkey"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T23:32:22.041210Z",
     "start_time": "2024-12-22T23:32:22.037043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eps = 10**(-8)\n",
    "\n",
    "def vectorized_value_estimation(state):\n",
    "    delta, v_pi, policy = state\n",
    "    expected_r = jnp.sum(policy * mdp.expected_rewards, axis=1)\n",
    "    expected_v = jnp.sum(policy[:, :, None] * mdp.transition_ps * v_pi[None, None, :], axis=(1,2)) \n",
    "    v_pi_new = expected_r + discount_factor * expected_v\n",
    "    delta = jnp.max(jnp.abs(v_pi_new - v_pi))\n",
    "    return (delta, v_pi_new, policy)\n",
    "\n",
    "def cond_function(state):\n",
    "    delta, v_pi, _ = state\n",
    "    return delta > eps\n",
    "\n",
    "def calculate_value_function(policy, v_pi):\n",
    "    init_state = (1e13, v_pi, policy)\n",
    "    delta, v_pi, _ = lax.while_loop(cond_function, vectorized_value_estimation, init_state)\n",
    "    return v_pi"
   ],
   "id": "1418b64072f13e74",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T23:32:22.204815Z",
     "start_time": "2024-12-22T23:32:22.052008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#policy matrix: num_states X num_actions with sum(policy[i,:]) == 1\n",
    "key, subkey = random.split(key)\n",
    "policy = random.uniform(subkey, [num_states, num_actions])\n",
    "del subkey\n",
    "\n",
    "policy_0 = policy / policy.sum(axis=-1, keepdims=True)\n",
    "\n",
    "#random value init\n",
    "key, subkey = random.split(key)\n",
    "v_pi_0 = random.uniform(subkey, num_states, dtype=jnp.float32)"
   ],
   "id": "b931cff8353dd5d1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Improve on the random policy by using iterative policy improvement:\n",
    "\n",
    "$$\\pi'(s) = \\text{argmax}_a \\ q_\\pi(s,a) = \\text{argmax}_a \\sum_{s',r} p(s',r|s,a) [r + \\gamma v_\\pi(s')] = \\text{argmax}_a [\\sum_{r} p(r|s,a) r +  \\gamma \\sum_{s'} p(s'|s,a) v_\\pi(s')]$$"
   ],
   "id": "f25146a85bd184aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T23:33:12.932853Z",
     "start_time": "2024-12-22T23:33:12.891294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_iters = 10\n",
    "\n",
    "policy = jnp.copy(policy_0)\n",
    "v_pi = jnp.copy(v_pi_0)\n",
    "\n",
    "for i in range(num_iters):\n",
    "    v_pi = calculate_value_function(policy, v_pi)\n",
    "    q_pi = mdp.expected_rewards + discount_factor * jnp.sum(mdp.transition_ps * v_pi[None, None, :], axis=-1)\n",
    "    best_actions = jnp.argmax(q_pi, axis=-1)\n",
    "    policy = jnn.one_hot(best_actions, num_actions) \n",
    "    \n",
    "    mean_value = jnp.mean(v_pi)\n",
    "    print(f'{i} - mean policy value {mean_value}')"
   ],
   "id": "2727f8845f028cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - mean policy value 16.9290771484375\n",
      "1 - mean policy value 47.43392562866211\n",
      "2 - mean policy value 47.47530746459961\n",
      "3 - mean policy value 47.47545623779297\n",
      "4 - mean policy value 47.47545623779297\n",
      "5 - mean policy value 47.47545623779297\n",
      "6 - mean policy value 47.47545623779297\n",
      "7 - mean policy value 47.47545623779297\n",
      "8 - mean policy value 47.47545623779297\n",
      "9 - mean policy value 47.47545623779297\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
